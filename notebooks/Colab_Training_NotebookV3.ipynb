{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru deep_translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbUOixvexMPQ",
        "outputId": "2f6ff90a-17ef-47c0-8c7b-5f27f972a975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep_translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep_translator) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.11.12)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, deep_translator\n",
            "Successfully installed deep_translator-1.11.4 loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 1. Setup: Mount Google Drive & Install Libraries\n",
        "# Purpose: This cell connects your Colab environment to your Google Drive\n",
        "# and installs the required 'transformers' library for BERT.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive. You'll be prompted to authorize this.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install the Hugging Face transformers library quietly\n",
        "!pip install -q transformers\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted and libraries installed.\")\n",
        "\n",
        "\n",
        "# @title ## 2. Clone Your GitHub Repository\n",
        "# Purpose: This cell downloads your code and data from GitHub into the Colab environment.\n",
        "\n",
        "# Define the repository URL\n",
        "repo_url = \"https://github.com/nhahub/NHA-112.git\"\n",
        "repo_name = \"NHA-112\"\n",
        "\n",
        "# Clone the repository\n",
        "if not os.path.exists(repo_name):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(\"Repository already cloned.\")\n",
        "\n",
        "# Change the working directory into your repository\n",
        "os.chdir(repo_name)\n",
        "print(f\"‚úÖ Repository cloned. Current working directory: {os.getcwd()}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted and libraries installed.\n",
            "Cloning into 'NHA-112'...\n",
            "remote: Enumerating objects: 815, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 815 (delta 2), reused 2 (delta 2), pack-reused 808 (from 1)\u001b[K\n",
            "Receiving objects: 100% (815/815), 98.38 MiB | 31.06 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n",
            "‚úÖ Repository cloned. Current working directory: /content/NHA-112\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGcAvNz5wa_G",
        "outputId": "8e72aff1-2dee-421f-9856-aa875d133b32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 3. Import Your BertTextClassifier Class\n",
        "# Purpose: This cell adds your repository's code to Python's path and imports\n",
        "# the necessary libraries and your custom classifier.\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add the current directory to the Python path to find your module\n",
        "sys.path.append('.')\n",
        "\n",
        "# Now you can import your class\n",
        "from dataEngineer.modeling.Deeplearning2 import *\n",
        "from dataEngineer.pipeLine import *\n",
        "\n",
        "print(\"‚úÖ SentimentAnalysisModel class imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBcfJ5BQwfEw",
        "outputId": "a162b46e-c956-4ec4-8e07-b349aa1c2a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SentimentAnalysisModel class imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/NHA-112/data/interim/reddit_complaints_dataset.csv\""
      ],
      "metadata": {
        "id": "89RCfAIy-mAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "7CfDXZAA-hhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].convert_dtypes('object')\n",
        "df = df[df['text'].str.strip() != \"\"]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "cleaning_pipeline = Pipeline([(\"text_preprocessor\", NltkTextPreprocessor())])\n",
        "\n",
        "print(\"Applying sklearn pipeline for text cleaning and lemmatization...\")\n",
        "processed_text_series = pd.Series(\n",
        "    cleaning_pipeline.fit_transform(df['text']), name=\"processed_text\"\n",
        ")\n",
        "df[\"processed_text\"] = processed_text_series"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFwNEOrgKQrQ",
        "outputId": "8a829be6-3aa3-4e8e-de63-b9d7351afec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying sklearn pipeline for text cleaning and lemmatization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "OHg2H-n9-Bkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4faa34-b27b-4e71-d353-a9a0cbadaec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['category', 'subreddit', 'problem_type', 'title', 'text',\n",
              "       'processed_text'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UiuU7u9rXFPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ‚ú® FUNCTION: Evaluate a model using your own class handler\n",
        "# ---------------------------------------------------------\n",
        "def evaluate_model(model_path, model_name, df, text_column, category_column, subcategory_column, batch_size=8):\n",
        "    print(f\"\\nüìå Evaluating model: {model_path}\")\n",
        "\n",
        "    handler = MultiOutputClassificationModel(model_name=model_name, model_path=model_path)\n",
        "\n",
        "    # Load model\n",
        "    model, optimizer = handler.load()\n",
        "    if model is None:\n",
        "        print(f\"‚ùå Cannot load model: {model_path}\")\n",
        "        return None\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Encode labels (same process used in training)\n",
        "    encoded_cat = handler.label_encoder_category.transform(df[category_column])\n",
        "    encoded_sub = handler.label_encoder_subcategory.transform(df[subcategory_column])\n",
        "\n",
        "    texts = df[text_column].values\n",
        "\n",
        "    # Train/Val split EXACT like training\n",
        "    train_texts, val_texts, train_cat, val_cat, train_sub, val_sub = train_test_split(\n",
        "        texts, encoded_cat, encoded_sub, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Build dataset + dataloader\n",
        "    val_dataset = MultiOutputDataset(val_texts, val_cat, val_sub, handler.tokenizer)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    all_preds_cat = []\n",
        "    all_preds_sub = []\n",
        "    all_labels_cat = []\n",
        "    all_labels_sub = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(handler.device)\n",
        "            attention_mask = batch['attention_mask'].to(handler.device)\n",
        "            labels_cat = batch['labels_category'].to(handler.device)\n",
        "            labels_sub = batch['labels_subcategory'].to(handler.device)\n",
        "\n",
        "            out_cat, out_sub = model(input_ids, attention_mask)\n",
        "\n",
        "            # Total loss\n",
        "            loss_cat = criterion(out_cat, labels_cat)\n",
        "            loss_sub = criterion(out_sub, labels_sub)\n",
        "            total_loss += (loss_cat + loss_sub).item()\n",
        "\n",
        "            _, pred_cat = torch.max(out_cat, 1)\n",
        "            _, pred_sub = torch.max(out_sub, 1)\n",
        "\n",
        "            all_preds_cat.extend(pred_cat.cpu().numpy())\n",
        "            all_preds_sub.extend(pred_sub.cpu().numpy())\n",
        "            all_labels_cat.extend(labels_cat.cpu().numpy())\n",
        "            all_labels_sub.extend(labels_sub.cpu().numpy())\n",
        "\n",
        "    acc_cat = accuracy_score(all_labels_cat, all_preds_cat)\n",
        "    acc_sub = accuracy_score(all_labels_sub, all_preds_sub)\n",
        "\n",
        "    results = {\n",
        "        \"model_path\": model_path,\n",
        "        \"val_loss\": total_loss / len(val_loader),\n",
        "        \"category_accuracy\": acc_cat,\n",
        "        \"subcategory_accuracy\": acc_sub,\n",
        "        \"avg_accuracy\": (acc_cat + acc_sub) / 2\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ‚ú® COMPARE MULTIPLE MODELS\n",
        "# ---------------------------------------------------------\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/bert_multiclassification_uncleanedData/sentiment_classifier.pth\",\n",
        "    \"/content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData/sentiment_classifier.pth\",\n",
        "    \"/content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData_50epoch_2e-6/sentiment_classifier.pth\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "for mp in model_paths:\n",
        "    res = evaluate_model(\n",
        "        model_path=mp,\n",
        "        model_name=\"distilbert-base-uncased\",\n",
        "        df=df,\n",
        "        text_column='processed_text',\n",
        "        category_column='category',\n",
        "        subcategory_column='problem_type'\n",
        "    )\n",
        "    if res:\n",
        "        results.append(res)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ‚ú® PRINT RESULTS SORTED BY BEST MODEL\n",
        "# ---------------------------------------------------------\n",
        "if results:\n",
        "    print(\"\\n\\n===================== MODEL COMPARISON =====================\")\n",
        "    results = sorted(results, key=lambda x: x['avg_accuracy'], reverse=True)\n",
        "\n",
        "    for r in results:\n",
        "        print(f\"\\nModel: {r['model_path']}\")\n",
        "        print(f\" - Validation Loss:        {r['val_loss']:.4f}\")\n",
        "        print(f\" - Category Accuracy:      {r['category_accuracy']*100:.2f}%\")\n",
        "        print(f\" - Sub-category Accuracy:  {r['subcategory_accuracy']*100:.2f}%\")\n",
        "        print(f\" - Overall Avg Accuracy:   {(r['avg_accuracy']*100):.2f}%\")\n",
        "\n",
        "    print(\"\\nüèÜ BEST MODEL:\")\n",
        "    print(results[0])\n",
        "else:\n",
        "    print(\"‚ùå No models could be evaluated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaYjpmHkVVO3",
        "outputId": "407b9fbe-aab7-4074-91da-b3fae4f10464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå Evaluating model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/bert_multiclassification_uncleanedData/sentiment_classifier.pth\n",
            "Using device: cuda\n",
            "‚úÖ Loaded model from /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/bert_multiclassification_uncleanedData/sentiment_classifier.pth.\n",
            "\n",
            "üìå Evaluating model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData/sentiment_classifier.pth\n",
            "Using device: cuda\n",
            "‚úÖ Loaded model from /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData/sentiment_classifier.pth.\n",
            "\n",
            "üìå Evaluating model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData_50epoch_2e-6/sentiment_classifier.pth\n",
            "Using device: cuda\n",
            "‚úÖ Loaded model from /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData_50epoch_2e-6/sentiment_classifier.pth.\n",
            "\n",
            "\n",
            "===================== MODEL COMPARISON =====================\n",
            "\n",
            "Model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/bert_multiclassification_uncleanedData/sentiment_classifier.pth\n",
            " - Validation Loss:        2.0444\n",
            " - Category Accuracy:      84.36%\n",
            " - Sub-category Accuracy:  58.34%\n",
            " - Overall Avg Accuracy:   71.35%\n",
            "\n",
            "Model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData_50epoch_2e-6/sentiment_classifier.pth\n",
            " - Validation Loss:        2.4550\n",
            " - Category Accuracy:      82.65%\n",
            " - Sub-category Accuracy:  58.08%\n",
            " - Overall Avg Accuracy:   70.37%\n",
            "\n",
            "Model: /content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/text_classifier_cleanedData/sentiment_classifier.pth\n",
            " - Validation Loss:        2.1876\n",
            " - Category Accuracy:      82.52%\n",
            " - Sub-category Accuracy:  57.95%\n",
            " - Overall Avg Accuracy:   70.24%\n",
            "\n",
            "üèÜ BEST MODEL:\n",
            "{'model_path': '/content/drive/MyDrive/DataEngineerProject_productSentementAnaylsis/bert_multiclassification_uncleanedData/sentiment_classifier.pth', 'val_loss': 2.0444264293958745, 'category_accuracy': 0.8436268068331143, 'subcategory_accuracy': 0.5834428383705651, 'avg_accuracy': 0.7135348226018396}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}